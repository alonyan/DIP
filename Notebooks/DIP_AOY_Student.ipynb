{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "disclaimer: To ensure that the notebook can be run from (more or less) any point, I try to load the relevant functions or modules whenever I use them in a cell. This is generally not good practice as it adds unneccesary overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.  Image representation as numerical arrays\n",
    "### We start by importing numpy and creating and printing a simple 9x9 checkerboard array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import numpy as np\n",
    "\n",
    "#make a 9x9 checkerboard\n",
    "checkBoard = #\n",
    "\n",
    "print(checkBoard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, we import [pyplot](https://matplotlib.org/api/pyplot_api.html) and [image](https://matplotlib.org/api/image_api.html) modules from the ploting library [matplotlib](https://matplotlib.org/3.1.1/api/index.html). Using it, we can display our checkerboard array in image form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "plt.imshow(checkBoard, cmap='gray', interpolation='nearest') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As another simple example, we will import the [data](https://scikit-image.org/docs/dev/api/skimage.data.html) module image processing library [scikit-image](https://scikit-image.org/) and load a small image of a bush. \n",
    "\n",
    "#### First, we want to print the pixel values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "image_of_a_bush = data.lfw_subset()\n",
    "image_of_a_bush = image_of_a_bush[0,:,:]\n",
    "\n",
    "#print the #of dimentions, the shape, and the pixel values of the image\n",
    "print(\"The number of dimensions of the image is: \", image_of_a_bush.ndim)\n",
    "print(\"The size of the image is: \", image_of_a_bush.shape)\n",
    "print(image_of_a_bush)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you see the bush? \n",
    "\n",
    "#### Next, show the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(1,1))\n",
    "\n",
    "# display the image\n",
    "plt.#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pixel-level operations \n",
    "### Now that we have a sense of what a digital image is, let's start manipulating it. We'll begin with simple pixel-level operations\n",
    "\n",
    "## 1.1 Basic pixel-level operations\n",
    "### Let's look at a more interesting image. From scikit-image data we'll open a example IHC image, and plot it using pyplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "image_hist = data.immunohistochemistry()\n",
    "#check the size of the image\n",
    "print(\"The number of dimensions of the image is: \", image_hist.ndim)\n",
    "print(\"The size of the image is: \", image_hist.shape)\n",
    "plt.imshow(image_hist, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seems like we have an RGB image. Let's look at every channel independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(131)\n",
    "plt.gca().set_title('Red channel')\n",
    "plt.imshow(, cmap='Reds', interpolation='nearest')\n",
    "plt.subplot(132)\n",
    "plt.gca().set_title('Green channel')\n",
    "plt.imshow(, cmap='Greens', interpolation='nearest')\n",
    "plt.subplot(133)\n",
    "plt.gca().set_title('Blue channel')\n",
    "plt.imshow(, cmap='Blues', interpolation='nearest')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the moment let's look at only the first color channel\n",
    "image_hist = image_hist[:,:,0]\n",
    "plt.gca().set_title('First channel')\n",
    "plt.imshow(image_hist, cmap=plt.cm.gray)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can invert the image using the *invert* function from [scikit-images utilities module](https://scikit-image.org/docs/dev/api/skimage.util.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.util import invert\n",
    "\n",
    "inverted_image = #\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.gca().set_title('original image')\n",
    "plt.imshow(image_hist, cmap=plt.cm.gray)\n",
    "plt.subplot(122)\n",
    "plt.gca().set_title('inverted image')\n",
    "plt.imshow(inverted_image, cmap=plt.cm.gray)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try some other pixel-level operations. We'll use the [Exposure module](https://scikit-image.org/docs/dev/api/skimage.exposure.html#skimage.exposure.adjust_sigmoid) from scikit image.\n",
    "\n",
    "1. A gamma correction applies the nonlinear transform $V_{out} = V_{in}^\\gamma$.\n",
    "\n",
    "2. A log transform applies $V_{out} = log(V_{in}+1)$.\n",
    "\n",
    "3. A sigmoid transform applies $V_{out} = \\frac{1}{1+e^{gain\\cdot(\\text{cutoff}-V_{in})}}$.\n",
    "\n",
    "4. Equalization transforms the intensity histogram of an image to a uniform distribution. It often enhances the contrast of the image\n",
    "\n",
    "5. Contrast Limited Adaptive Histogram Equalization (CLAHE) works similarly to equalization thats applied separately to different regions of the image.\n",
    "\n",
    "Try to apply these by calling the relevant function from skimage.exposure, or by direct calculation. \n",
    "\n",
    "Play with the different parameters and see how they change the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import exposure\n",
    "\n",
    "# apply gamma scaling with gamma=2\n",
    "gamma=\n",
    "gamma_corrected = \n",
    "\n",
    "# apply logarithmic scaling\n",
    "logarithmic_corrected = \n",
    "\n",
    "# apply sigmoidal scaling with cutoff=0.4\n",
    "cutoff = \n",
    "sigmoid_corrected = \n",
    "\n",
    "# equalize\n",
    "equalize_corrected = \n",
    "\n",
    "# apply Contrast Limited Adaptive Histogram Equalization (CLAHE)\n",
    "CLHA_corrected = \n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.subplot(231)\n",
    "plt.gca().set_title('original')\n",
    "plt.imshow(image_hist, cmap=plt.cm.gray)\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.gca().set_title('gamma corrected')\n",
    "plt.imshow(gamma_corrected, cmap=plt.cm.gray)\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.gca().set_title('log corrected')\n",
    "plt.imshow(logarithmic_corrected, cmap=plt.cm.gray)\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.gca().set_title('sigmoid')\n",
    "plt.imshow(sigmoid_corrected, cmap=plt.cm.gray)\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.gca().set_title('equalized')\n",
    "plt.imshow(equalize_corrected, cmap=plt.cm.gray)\n",
    "\n",
    "plt.subplot(236)\n",
    "plt.gca().set_title('CLHA corrected')\n",
    "plt.imshow(CLHA_corrected, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Image filtering\n",
    "\n",
    "### Spatial filtering is an image processing technique for changing the intensities of a pixel according to the intensities of some neighborhood of pixels. \n",
    "<img src=\"./images/same_padding_no_strides.gif\" width=\"400\" height=\"200\" >\n",
    "\n",
    "\n",
    "### The *Kernel* of the filter defines the neighborhood and the weights asigned to each pixel in the neighborhood:\n",
    "\n",
    "<img src=\"./images/spatialFilter.jpg\" width=\"400\" height=\"200\" >\n",
    "This procedure is formally a convolution and is marked by an asterisk: $I_o = I_i\\ast f$. \n",
    "\n",
    "*side note: since a convolution in the spatial domain is equivalent to multiplication in the frequency domain. Sometimes it is more computationally reasonable to calculate these in fourier space.*\n",
    "\n",
    "*side side note: filtering can also be performed in the frequency domain by directly removing a set of frequencies from an image.*\n",
    "\n",
    "\n",
    "### The kernel can be of any shape/size, it is applied to each pixel in the image, and the output is a new, filtered, image. The output image is often called the *response* to the given filter.\n",
    "Example, local average: <img src=\"./images/spatialFilterExample.jpg\" width=\"300\" height=\"150\">\n",
    "\n",
    "\n",
    "#### Filtering is an incredibly versatile tool with which you can emphasize certain features or remove other features. \n",
    "#### Image processing operations implemented with filtering include smoothing, sharpening, and edge enhancement.\n",
    "\n",
    "\n",
    "### To implement different image filters, we will use the [filters module from scikit-image](https://scikit-image.org/docs/dev/api/skimage.filters.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Smoothing\n",
    "\n",
    "#### Smoothing, aka low-pass filtering, is used for removing high-frequency noise from images. Most commonly, a gaussian kernel is used, but others (e.g. local mean/median) work too. We'll see the effect of gaussian filtering.\n",
    "\n",
    "Try to change the value of sigma (width of the gaussian) to see how the output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from skimage import filters\n",
    "\n",
    "image_hist = data.immunohistochemistry()\n",
    "\n",
    "sigma = 2\n",
    "\n",
    "gauss_filtered_img = \n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(121)\n",
    "plt.gca().set_title('original image')\n",
    "plt.imshow(image_hist, cmap=plt.cm.gray)\n",
    "plt.subplot(122)\n",
    "plt.gca().set_title('response, gaussian smoothing')\n",
    "plt.imshow(gauss_filtered_img, cmap=plt.cm.gray)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Sharpening\n",
    "\n",
    "#### sharpening is sometimes used to enhance a blurry (i.e. crappy) image. \n",
    "\n",
    "\n",
    "1. Start with input image\n",
    "2. Apply gaussian filter with very narrow kernel\n",
    "3. Subtract filtered image from input image to get only high frequency components\n",
    "3. Amplify (alpha) and add high frequency components to original input image \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_blurred_f = filters.gaussian(gauss_filtered_img, sigma=0.5, multichannel=False)\n",
    "alpha = 3\n",
    "sharpened = gauss_filtered_img + alpha * (gauss_filtered_img - filter_blurred_f)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(121)\n",
    "plt.gca().set_title('input - blury image')\n",
    "plt.imshow(gauss_filtered_img, cmap=plt.cm.gray)\n",
    "plt.subplot(122)\n",
    "plt.gca().set_title('sharpened')\n",
    "plt.imshow(sharpened, cmap=plt.cm.gray)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Direct application of edge detectors often results in somewhat noisy responses. A way to overcome this is by first smoothing the image with a gaussian filter and then applying the edge filter. The width of the gaussian kernel will determine the size of edges the filter detects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Edge enhancement\n",
    "\n",
    "#### Edge detecting filters work by measuring the local spatial gradient of an image. Common types are the Sobel, Prewitt and Roberts. \n",
    "\n",
    "#### The filters are usually applied to each direction individually and then the total magnitude of the gradient is calculated.\n",
    "$|\\nabla| = \\sqrt{\\nabla_x^2+\\nabla_y^2}$\n",
    "\n",
    "#### Sobel: \n",
    "<img src=\"./images/sobmasks.gif\" width=\"200\" height=\"100\" align=\"left\">\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "#### Prewitt: \n",
    "<img src=\"./images/premasks.png\" width=\"200\" height=\"100\" align=\"left\">\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "#### Roberts: \n",
    "<img src=\"./images/robmasks.gif\" width=\"200\" height=\"100\" align=\"left\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "image_hist = data.immunohistochemistry()\n",
    "image_hist = image_hist[:,:,2]\n",
    "\n",
    "\n",
    "# sobel magnitude\n",
    "filtered_img = \n",
    "# sobel horizontal\n",
    "filtered_img_h = \n",
    "# sobel vertical\n",
    "filtered_img_v = \n",
    "\n",
    "plt.figure(figsize=(15,16))\n",
    "plt.subplot(221)\n",
    "plt.gca().set_title('input image')\n",
    "plt.imshow(image_hist[:,:,2], cmap=plt.cm.gray)\n",
    "plt.subplot(222)\n",
    "plt.gca().set_title('sobel filter response - magnitude')\n",
    "plt.imshow(filtered_img, cmap=plt.cm.gray)\n",
    "plt.subplot(223)\n",
    "plt.gca().set_title('sobel filter response - horizontal edges')\n",
    "plt.imshow(np.abs(filtered_img_h), cmap=plt.cm.gray)\n",
    "plt.subplot(224)\n",
    "plt.gca().set_title('sobel filter response - vertical edges')\n",
    "plt.imshow(np.abs(filtered_img_v), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Direct application of edge detectors often results in somewhat noisy responses. A way to overcome this is by first smoothing the image with a gaussian filter and then applying the edge filter. The width of the gaussian kernel will determine the size of edges the filter detects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "from skimage import feature\n",
    "\n",
    "image_hist = data.immunohistochemistry()\n",
    "image_hist = image_hist[:,:,2]\n",
    "\n",
    "# sobel magnitude\n",
    "filtered_img = filters.sobel(image_hist)\n",
    "\n",
    "#apply a gaussian filter, followed by a sobel filter with sigma=3\n",
    "sigma=\n",
    "DoG_img = \n",
    "\n",
    "plt.figure(figsize=(15,16))\n",
    "plt.subplot(221)\n",
    "plt.gca().set_title('input image')\n",
    "plt.imshow(image_hist, cmap=plt.cm.gray)\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.gca().set_title('sobel filter response - magnitude')\n",
    "plt.imshow(filtered_img, cmap=plt.cm.gray)\n",
    "plt.subplot(224)\n",
    "plt.gca().set_title('DoG filter response - magnitude')\n",
    "plt.imshow(DoG_img, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Gaussian derivatives \n",
    "Now let's remember that the application of filters is an associative operation (because convolution is linear!). It's equivalent to apply the gaussian and then the gradient filter to the image and to apply the gradient filter to the gaussian, then apply the result to the image, i.e. $\\nabla*(G*I) = (\\nabla*G)*I$ where $\\nabla$ is the gradient (derivative) filter, $G$ is the gaussian filter, and $I$ is the image.\n",
    "\n",
    "We can generalize this idea and apply the derivative multiple times, to get a family of interesting filters:\n",
    "\n",
    "$\\nabla^n*G$:\n",
    "\n",
    "<img src=\"./images/GaussianDerivs.png\" width=\"800\" height=\"200\">\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Notice that all of the even orders are edge detectors, while all the odd orders are ridge detectors!\n",
    "\n",
    "### Let's use a second order derivative (aka a Laplacian) and make a ridge detector:\n",
    "#### We'll look at a retinal photo where vasculature is an interesting feature. We invert the image so that the vasculature appears as bright lines on a dark background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import img_as_float\n",
    "import matplotlib.image as mpimg\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.util import invert\n",
    "\n",
    "#this is how we load an image from the hard drive\n",
    "image_retina = ((img_as_float(mpimg.imread(\"../Data/RetinalPhoto.png\"))))\n",
    "image_ridges = invert(rgb2gray(image_retina))\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.gca().set_title('retinal photo')\n",
    "plt.imshow(image_retina, cmap=plt.cm.gray)\n",
    "plt.subplot(122)\n",
    "plt.gca().set_title('inverted grayscale image')\n",
    "plt.imshow(image_ridges, cmap=plt.cm.gray)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply a gaussian filter, followed by a second order derivative (laplacian).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma=3\n",
    "LoG_ridges = \n",
    "\n",
    "plt.imshow(LoG_ridges,cmap='gray')\n",
    "plt.gca().set_title('retinal photo - response to LoG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This commonly used ridge detector is called a Laplacian of Gaussian (LoG)!**\n",
    "\n",
    "(We'll get back to this image in a bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Masking\n",
    "\n",
    "### A mask is a binary image (0s and 1s) that typically separates a given input image into Foreground (interesting) and Background (boring) regions, or for picking a region-of-interest (ROI). A mask is *applied* to an image by element-wise multiplication. The size of a mask must be *identical* to the size of the image it's applied to.\n",
    "\n",
    "\n",
    "### Let's begin by creating a simple circular mask. We'll create an array where the value at each point is it's distance from the center of the image, and display it as an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#dimensions in x and y\n",
    "y = 512\n",
    "x = 512\n",
    "\n",
    "#position of center\n",
    "centY = np.ceil(y/2)\n",
    "centX = np.ceil(x/2)\n",
    "\n",
    "#create the grid\n",
    "yy,xx = np.indices((y,x))\n",
    "\n",
    "#create radial distance map\n",
    "radialDist = \n",
    "\n",
    "#display\n",
    "plt.gca().set_title('Radial distance')\n",
    "plt.imshow(radialDist, cmap='gray', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Of these points, we'll pick a circle of radius 100 and display it as an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circ1 = \n",
    "\n",
    "plt.show()\n",
    "plt.gca().set_title('Circle with radius 100')\n",
    "plt.imshow(circ1, cmap='inferno', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This object is a **mask**. If you multiply this matrix of 0s and 1s with an image of the same size, only the parts that are ==1 will remain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's apply this mask to our histology image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gca().set_title('Masked first channel')\n",
    "plt.imshow(, cmap=plt.cm.gray)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens if we invert the mask?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inverted_mask = \n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.gca().set_title('inverted mask')\n",
    "plt.imshow(inverted_mask, cmap=plt.cm.gray)\n",
    "plt.subplot(122)\n",
    "plt.gca().set_title('inverted masked image')\n",
    "plt.imshow(image_hist[:,:,2]*, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Just for closure, let's see what happens when we look at the full RGB image and try to apply the mask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = data.immunohistochemistry()\n",
    "\n",
    "masked_image = image*circ1\n",
    "plt.imshow(masked_image, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Whoops. Seems like something is wrong. Our problem is that numpy didn't know how to multiply a 512x512x3 with a 512x512 mask. Numpy makes solving this very easy by adding a singleton dimension (look up broadcasting in your spare time).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = data.immunohistochemistry()\n",
    "plt.gca().set_title('Masked image')\n",
    "masked_image = mage*np.expand_dims(circ1,2)\n",
    "plt.imshow(masked_image, cmap=plt.cm.gray)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Thresholding\n",
    "## 1.4.1 Simple thresholding\n",
    "### Thresholding an image is the process of setting an intensity (or intensities) for separating the different components of an image.\n",
    "\n",
    "<img src=\"./images/Thresholding.png\" width=\"600\" height=\"600\" >\n",
    "\n",
    "\n",
    "#### In simplest case, the foreground and background have very different intensities. In that case thresholding is just clustering pixels by their intensity levels. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function from skimage converts images of integer types into floats, which are easier to work with.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import img_as_float\n",
    "from skimage import data\n",
    "\n",
    "# First, let's create a noisy image of blobs\n",
    "image_blobs = img_as_float(data.binary_blobs(length=512, seed=1))\n",
    "sigma = 0.22\n",
    "image_blobs += np.random.normal(loc=0, scale=sigma, size=image_blobs.shape)\n",
    "\n",
    "print(\"The number of dimensions of the image is: \", image_blobs.ndim)\n",
    "print(\"The size of the image is: \", image_blobs.shape)\n",
    "plt.imshow(image_blobs, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To find the right threshold, let's examine a histogram of pixel intensity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(image_blobs.flatten(),bins=250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick an appropriate threshold, by eye, and see if you can remove the background. \n",
    "What happens when you increase or decrease the threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = \n",
    "\n",
    "mask = \n",
    "\n",
    "masked_image = \n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(131)\n",
    "plt.gca().set_title('original')\n",
    "plt.imshow(image_blobs, interpolation='nearest', cmap=plt.cm.gray)\n",
    "plt.subplot(132)\n",
    "plt.gca().set_title('mask')\n",
    "plt.imshow(mask, interpolation='nearest', cmap=plt.cm.gray)\n",
    "plt.subplot(133)\n",
    "plt.gca().set_title('masked image')\n",
    "plt.imshow(masked_image, interpolation='nearest', cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our mask looks ok, but it has a lot of salt & pepper speckle noise. Why is that?\n",
    "We can try and use what we learned before about filtering to clean up our results. What filter should we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "\n",
    "thresh = \n",
    "\n",
    "mask = \n",
    "\n",
    "masked_image = \n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(131)\n",
    "plt.gca().set_title('original')\n",
    "plt.imshow(image_blobs, interpolation='nearest', cmap=plt.cm.gray)\n",
    "plt.subplot(132)\n",
    "plt.gca().set_title('mask')\n",
    "plt.imshow(mask, interpolation='nearest', cmap=plt.cm.gray)\n",
    "plt.subplot(133)\n",
    "plt.gca().set_title('masked image')\n",
    "plt.imshow(masked_image, interpolation='nearest', cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's usually a good idea before creating a mask to despeckle an image using a narrow gaussian filter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.2 Morphological operations\n",
    "Morphology is a broad set of image processing operations that process images based on shapes. In a morphological operation, each pixel in the image is adjusted based on the value of other pixels in its neighborhood. By choosing the size and shape of the neighborhood, you can construct a morphological operation that is sensitive to specific shapes in the input image. (explanation from Mathworks)\n",
    "\n",
    "Morphological operations are based around a *structuring element*, which is a small binary image, often of a disk or a square. The structuring element is positioned at all possible locations in the image and it is compared with the corresponding neighbourhood of pixels. Some operations test whether the element \"fits\" within the neighbourhood, while others test whether it \"hits\" or intersects the neighbourhood.\n",
    "\n",
    "Common operations for image processing\n",
    "\n",
    "Erosion - output image =1 wherever the structuring element **fits** (erodes the mask)\n",
    "\n",
    "Dilation - output image =1 wherever the structuring element **hits** (expands the mask)\n",
    "\n",
    "Opening - Erosion followed by dilation (opens gaps in spots where the mask is weakly connected)\n",
    "\n",
    "Closing - Dilation followed by erosion (closes holes in the mask)\n",
    "\n",
    "\n",
    "A very thorough explanation of morphological operationscould be found [here](https://www.cs.auckland.ac.nz/courses/compsci773s1c/lectures/ImageProcessing-html/topic4.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import erosion, dilation, opening, closing\n",
    "from skimage.morphology import disk\n",
    "\n",
    "#define a \"disk\" structuring element with radius 10\n",
    "selem = \n",
    "\n",
    "#apply erosion, dilation, opening, and closing\n",
    "erosion_mask = \n",
    "\n",
    "dilation_mask = \n",
    "\n",
    "opening_mask = \n",
    "\n",
    "closing_mask = \n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(231)\n",
    "plt.gca().set_title('mask')\n",
    "plt.imshow(mask, interpolation='nearest', cmap=plt.cm.gray)\n",
    "plt.subplot(232)\n",
    "plt.gca().set_title('erosion')\n",
    "plt.imshow(erosion_mask, interpolation='nearest', cmap=plt.cm.gray)\n",
    "plt.subplot(233)\n",
    "plt.gca().set_title('dilation')\n",
    "plt.imshow(dilation_mask, interpolation='nearest', cmap=plt.cm.gray)\n",
    "plt.subplot(235)\n",
    "plt.gca().set_title('opening')\n",
    "plt.imshow(opening_mask, interpolation='nearest', cmap=plt.cm.gray)\n",
    "plt.subplot(236)\n",
    "plt.gca().set_title('closing')\n",
    "plt.imshow(closing_mask, interpolation='nearest', cmap=plt.cm.gray)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.3 Masking actual data\n",
    "\n",
    "### We'll repeat the thresholding procedure using an actual microscopy image of fluorescent nuclei "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import img_as_float\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "#this is how we load an image from the hard drive\n",
    "image_nuclei = img_as_float(mpimg.imread(\"../Data/xy040-1.png\"))\n",
    "\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(7.1, 4.6), dpi=80, facecolor='w', edgecolor='k')\n",
    "print(\"The number of dimensions of the image is: \", image_nuclei.ndim)\n",
    "print(\"The size of the image is: \", image_nuclei.shape)\n",
    "plt.imshow(image_nuclei, cmap=plt.cm.gray, vmin=0, vmax=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again, let's plot a histogram of intensity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.\n",
    "plt.xlim((0, 0.02))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And again we'll pick a value by eye:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = \n",
    "\n",
    "#remember to despeckle before creating a mask!\n",
    "mask = \n",
    "\n",
    "plt.figure(figsize=(8,15))\n",
    "plt.subplot(311)\n",
    "plt.gca().set_title('original')\n",
    "plt.imshow(image_nuclei, interpolation='nearest', cmap=plt.cm.gray, vmin=0, vmax=0.01)\n",
    "plt.subplot(312)\n",
    "plt.gca().set_title('mask')\n",
    "plt.imshow(mask, interpolation='nearest', cmap=plt.cm.gray)\n",
    "plt.subplot(313)\n",
    "plt.gca().set_title('masked image')\n",
    "plt.imshow(image_nuclei*mask, interpolation='nearest', cmap=plt.cm.gray, vmin=0, vmax=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not bad! But also not very scalable. If we have 100s of images we can't look at them one-by-one and find thresholds by eye. Next, we'll look at some methods for automatically finding the thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Automated threshold calculation\n",
    "\n",
    "### There is a very large list of algorithms for threshold calculation out there that are optimized for different situations. We will briefly review a few of the most common ones.\n",
    "\n",
    "### 1.5.1 Iterative mean thresholding\n",
    "Algorithm:\n",
    "1. Start with some threshold $T_i$\n",
    "2. Compute the means $m_0$ and $m_1$ of the BG and FG\n",
    "3. Update $T_{i+1} = \\frac{m_0+m_1}{2}$\n",
    "4. Repeat until it converges\n",
    "\n",
    "<img src=\"./images/meanThresh.gif\" width=\"400\" height=\"400\" >\n",
    "\n",
    "### 1.5.2 Otsu thresholding\n",
    "The algorithm exhaustively searches for the threshold that minimizes the intra-class variance, defined for a given threshold $T$ as a weighted sum of variances of the two classes:\n",
    "$\\sigma^2_w(T)=\\omega_0(T)\\sigma^2_0(T)+\\omega_1(T)\\sigma^2_1(T)$\n",
    "\n",
    "For 2 classes, minimizing the intra-class variance is equivalent to maximizing inter-class variance, which is much easier to calculate:\n",
    "\\begin{align}\n",
    "\\sigma^2_b(T) & =\\sigma^2-\\sigma^2_w(T)=\\omega_0(\\mu_0-\\mu_T)^2+\\omega_1(\\mu_1-\\mu_T)^2 \\\\\n",
    "& =\\omega_0(T) \\omega_1(T) \\left[\\mu_0(T)-\\mu_1(T)\\right]^2\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"./images/Otsu's_Method_Visualization.gif\" width=\"400\" height=\"400\" >\n",
    "\n",
    "### 1.5.3 Triangle thresholding\n",
    "Algorithm:\n",
    "1. Draw a straight line between the histogram peak and the brightest value.\n",
    "2. From every point on that line, draw the shortest connecting line to the histogram.\n",
    "3. Find longest of these connecting lines.\n",
    "4. Threshold is set at the intersection of that line and the curve.\n",
    "<img src=\"./images/triThresh.png\" width=\"400\" height=\"400\" >\n",
    "\n",
    "*note: Triangle thresholding is good for situations where the image is mostly background, and there is no clear \"peak\" of bright pixels.*\n",
    "\n",
    "\n",
    "### [scikit-image's filters module](https://scikit-image.org/docs/dev/api/skimage.filters.html) implements a large variety of thresholding algorithms. Let's apply the ones we just learned about.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "\n",
    "#calculate iterative mean threshold\n",
    "meanThresh = \n",
    "print(meanThresh)\n",
    "\n",
    "#calculate otsu threshold\n",
    "OtsuThresh = \n",
    "print(OtsuThresh)\n",
    "\n",
    "#calculate triangle\n",
    "TriThresh = \n",
    "print(TriThresh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the resulting masks we get with each of these thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(num=None, figsize=(12, 8), dpi=80)\n",
    "ax1 = fig.add_axes([0.1,0.6,0.4,0.4])\n",
    "\n",
    "ax1.hist(image_nuclei.flatten(),bins=250)\n",
    "\n",
    "ax1.axvline(meanThresh, color='g', linestyle='--')\n",
    "ax1.axvline(OtsuThresh, color='r', linestyle='--')\n",
    "ax1.axvline(TriThresh, color='k', linestyle='--')\n",
    "\n",
    "ax1.legend(['mean' ,'otsu', 'triangle'])\n",
    "ax1.set_title('histogram')\n",
    "\n",
    "ax2 = fig.add_axes([0.6,0.6,0.4,0.4])\n",
    "#get iterative mean mask (remember to despeckle)\n",
    "mask_mean = \n",
    "\n",
    "ax2.imshow(mask_mean)\n",
    "ax2.set_title('Iterative mean')\n",
    "ax2.set_axis_off()\n",
    "\n",
    "ax2 = fig.add_axes([0.1,0.1,0.4,0.4])\n",
    "#get otsu mask\n",
    "mask_otsu = \n",
    "\n",
    "ax2.imshow(mask_otsu)\n",
    "ax2.set_title('Otsu')\n",
    "ax2.set_axis_off()\n",
    "\n",
    "ax2 = fig.add_axes([0.6,0.1,0.4,0.4])\n",
    "#get triangle mask\n",
    "mask_tri = \n",
    "\n",
    "ax2.imshow(mask_tri)\n",
    "ax2.set_title('Triangle')\n",
    "ax2.set_axis_off()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's briefly look back at the retinal images and try to mask only the vasculature:\n",
    "\n",
    "**as before, load the image, invert, and apply LoG filter:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import img_as_float\n",
    "import matplotlib.image as mpimg\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "#this is how we load an image from the hard drive\n",
    "image_ridges = invert(rgb2gray(img_as_float(mpimg.imread(\"../Data/RetinalPhoto.png\"))))\n",
    "\n",
    "plt.imshow(image_ridges, cmap=plt.cm.gray)\n",
    "plt.gca().set_title('retinal photo')\n",
    "sigma = 3\n",
    "LoG_ridges = filters.laplace(filters.gaussian(image_ridges, sigma=sigma))\n",
    "ax2.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, let's do the same procedure of automatically finding thresholds:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "\n",
    "meanThresh = filters.threshold_mean(LoG_ridges)\n",
    "print(meanThresh)\n",
    "\n",
    "OtsuThresh = filters.threshold_otsu(LoG_ridges)\n",
    "print(OtsuThresh)\n",
    "\n",
    "TriThresh = filters.threshold_triangle(LoG_ridges)\n",
    "print(TriThresh)\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(12, 8), dpi=80)\n",
    "ax1 = fig.add_axes([0.1,0.6,0.4,0.4])\n",
    "\n",
    "ax1.hist(image_nuclei.flatten(),bins=250)\n",
    "\n",
    "ax1.axvline(meanThresh, color='g', linestyle='--')\n",
    "ax1.axvline(OtsuThresh, color='r', linestyle='--')\n",
    "ax1.axvline(TriThresh, color='k', linestyle='--')\n",
    "ax1.legend(['mean' ,'otsu', 'triangle'])\n",
    "ax1.set_title('histogram')\n",
    "\n",
    "ax1 = fig.add_axes([0.2,0.65,0.3,0.3])\n",
    "\n",
    "plt.imshow(image_ridges, cmap=plt.cm.gray)\n",
    "ax1.set_axis_off()\n",
    "\n",
    "\n",
    "ax2 = fig.add_axes([0.6,0.6,0.4,0.4])\n",
    "mask_mean = \n",
    "\n",
    "ax2.imshow(mask_mean)\n",
    "ax2.set_title('Iterative mean')\n",
    "ax2.set_axis_off()\n",
    "\n",
    "ax2 = fig.add_axes([0.1,0.1,0.4,0.4])\n",
    "mask_otsu = \n",
    "\n",
    "ax2.imshow(mask_otsu)\n",
    "ax2.set_title('Otsu')\n",
    "ax2.set_axis_off()\n",
    "\n",
    "ax2 = fig.add_axes([0.6,0.1,0.4,0.4])\n",
    "mask_tri = \n",
    "\n",
    "ax2.imshow(mask_tri)\n",
    "ax2.set_title('Triangle')\n",
    "ax2.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Play around with the width of the gaussian. Which thresholding algorithm works best in this case?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.4 Local thresholding\n",
    "#### All of the methods we saw so far are *global* in the sense that the same threshold is applied to the whole picture. Sometimes we can have an image with vastly different intensity distributions at different locations. Using local thresholding, we can overcome such cases.\n",
    "\n",
    "Let's compare the results from a global (Otsu) and a local threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "\n",
    "image = data.page()\n",
    "fig = plt.figure(num=None, figsize=(12, 8), dpi=80)\n",
    "\n",
    "#global thresholding\n",
    "threshGlobal = filters.threshold_otsu(image)\n",
    "\n",
    "ax1 = fig.add_axes([0.1,0.6,0.4,0.4])\n",
    "ax1.set_title('mask - Otsu threshold')\n",
    "plt.imshow(image ,cmap='gray')\n",
    "\n",
    "ax2 = fig.add_axes([0.6,0.6,0.4,0.4])\n",
    "ax2.set_title('mask - Otsu threshold')\n",
    "plt.imshow(image>threshGlobal,cmap='gray')\n",
    "\n",
    "#local thresholding\n",
    "\n",
    "#Try and change this number and see what happens\n",
    "block_size  = 81\n",
    "\n",
    "#calculate local threshold map\n",
    "threshLocal = \n",
    "\n",
    "ax1 = fig.add_axes([0.1,0.2,0.4,0.4])\n",
    "ax1.imshow(threshLocal,cmap='gray')\n",
    "ax1.set_title('local threshold map')\n",
    "\n",
    "\n",
    "ax2 = fig.add_axes([0.6,0.2,0.4,0.4])\n",
    "ax2.set_title('mask - Local threshold')\n",
    "plt.imshow(image>threshLocal,cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Image segmentation\n",
    "### Image segmentation is the process of partitioning a digital image into multiple segments. The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze.\n",
    "<img src=\"./images/imageSegmentation.png\" width=\"400\" height=\"400\" >\n",
    "\n",
    "## 2.1 Connected components\n",
    "### After we generate a mask, the simplest segmentation is achieved by taking regions in the mask that are connected and labeling each one as a separate object. \n",
    "\n",
    "#### We begin by generating a simple mask using the triangle threshold method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import img_as_float\n",
    "import matplotlib.image as mpimg\n",
    "from skimage import filters\n",
    "\n",
    "image_nuclei = img_as_float(mpimg.imread(\"../Data/xy040-1.png\"))\n",
    "TriThresh = filters.threshold_triangle(image_nuclei)\n",
    "#despeckle\n",
    "mask = filters.gaussian(image_nuclei, sigma=1)>TriThresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [scikit-image's measure module](https://scikit-image.org/docs/dev/api/skimage.measure.html) implements a  variety of useful methods for segmentation. the *label* function returns a *labeled image* of connected components (CCs). Each CC is uniquely numbered by an integer.\n",
    "\n",
    "$\\begin{bmatrix} \n",
    "1 & 1 & 0 & 0 & 2\\\\\n",
    "1 & 1 & 0 & 2 & 2\\\\\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 3 & 0 & 4 & 4\\\\\n",
    "0 & 0 & 0 & 4 & 4\\\\\n",
    "\\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "#generate a labeled matrix of connected components\n",
    "labels = \n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(mask, cmap='gray')\n",
    "plt.subplot(122)\n",
    "plt.imshow(labels, cmap='nipy_spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can easily generate a *mask* for a **specific** CC using the binary operation *labels==i*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=43\n",
    "mask_of_CC_i = \n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(mask_of_CC_i, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem with simple CC segmentation : overlapping objects\n",
    "\n",
    "### We often really care about having only a single object per label. Using CC, any overlapping objects will merge into one blob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=87\n",
    "mask_of_CC_i = labels==i\n",
    "plt.imshow(mask_of_CC_i, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These problems can be partially resolved using morphological operations, but there's no silver bullet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import erosion, dilation, opening, closing\n",
    "from skimage.morphology import disk\n",
    "\n",
    "#define a \"disk\" structuring element\n",
    "selem1 = disk(10)\n",
    "selem2 = disk(7)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(121)\n",
    "plt.gca().set_title('original')\n",
    "plt.imshow(mask, cmap='nipy_spectral')\n",
    "plt.subplot(122)\n",
    "plt.gca().set_title('opening')\n",
    "plt.imshow(dilation(erosion(mask, selem1),selem2), interpolation='nearest', cmap='nipy_spectral')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Watershed based segmentation\n",
    "\n",
    "### 2.2.1 The watershed algorithm\n",
    " The watershed transformation treats the image it operates upon like a topographic map, with the brightness of each point representing its height, and finds the lines that run along the tops of ridges.\n",
    "\n",
    "<img src=\"./images/Diagram-of-watershed-algorithm.png\" width=\"400\" height=\"400\" >\n",
    "\n",
    "More precisely, the algorithm goes as follows:\n",
    "1. *Label* local minima (i.e. $S_1$, $S_2$)\n",
    "2. Move to next higher intensity level\n",
    "3. Assign to each point the label of it's closest label set.\n",
    "**<font color='red'>By passing the argument *watershed_line = 1* Points equidistant to multiple sets are labeled as boundaries and intensities set to 0</font>**\n",
    "4. Repeat until all points are labeled\n",
    "\n",
    "<img src=\"./images/watershed1.png\" width=\"400\" height=\"400\" >\n",
    "\n",
    "Let's start with a very naive application. We will invert the image, and then simply apply the *watershed* function from the [scikit-image morphology module](https://scikit-image.org/docs/dev/api/skimage.morphology). The function returns a labeled image. We'll plot the edges of that image using a Sobel filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import img_as_float\n",
    "import matplotlib.image as mpimg\n",
    "from skimage import filters\n",
    "\n",
    "from skimage.util import invert\n",
    "from skimage.morphology import watershed\n",
    "\n",
    "\n",
    "image_nuclei = img_as_float(mpimg.imread(\"../Data/xy040-1.png\"))\n",
    "\n",
    "#invert image\n",
    "inverted_image = \n",
    "\n",
    "image_to_watershed = inverted_image\n",
    "\n",
    "#Calculate watershed transform, remember to pass the  watershed_line = 1 argument\n",
    "labels_naive = \n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "#let's look at all the boundaries\n",
    "plt.figure(figsize=(12,20))\n",
    "plt.subplot(211)\n",
    "plt.gca().set_title('image fed to watershed')\n",
    "plt.imshow(image_to_watershed, cmap='gray')\n",
    "plt.subplot(212)\n",
    "plt.gca().set_title('watershed result')\n",
    "plt.imshow(filters.sobel(labels_naive), cmap='nipy_spectral')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So this clearly didn't work. Why? How do we fix it?\n",
    "\n",
    "Noise generates a ton of local minima. Each gets its own basin. This leads to massive oversegmentation.\n",
    "\n",
    "#### Watershed segmentation is only a *part* of a segmentation pipeline. Preprocessing (denoising, smoothing, seeding minima) of the image is CRUCIAL for it to work well. \n",
    "\n",
    "<img src=\"./images/PreprocessingApproaches.png\" width=\"600\" height=\"400\" >\n",
    "\n",
    "The first thing we'll do is to apply the mask that we found before. This is simply done by adding a *mask* argument to the watershed function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import img_as_float\n",
    "import matplotlib.image as mpimg\n",
    "from skimage import filters\n",
    "\n",
    "from skimage.util import invert\n",
    "from skimage.morphology import watershed\n",
    "\n",
    "image_nuclei = img_as_float(mpimg.imread(\"../Data/xy040-1.png\"))\n",
    "\n",
    "#calculate mask\n",
    "mask = filters.gaussian(image_nuclei, sigma=1)>TriThresh\n",
    "\n",
    "#apply mask and invert image\n",
    "masked_image = \n",
    "\n",
    "inverted_masked_image = \n",
    "\n",
    "image_to_watershed = inverted_masked_image\n",
    "\n",
    "#Calculate watershed transform\n",
    "#Now, also pass the mask to the watershed function so it avoids segmenting the BG\n",
    "labels_masked = \n",
    "\n",
    "#let's look at all the boundaries\n",
    "plt.figure(figsize=(12,20))\n",
    "plt.subplot(211)\n",
    "plt.gca().set_title('image fed to watershed')\n",
    "plt.imshow(image_to_watershed, cmap='gray')\n",
    "plt.subplot(212)\n",
    "plt.gca().set_title('watershed result')\n",
    "plt.imshow(filters.sobel(labels_masked), cmap='nipy_spectral')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So we got rid of all the BG regions, but we are still oversegmenting. Why?\n",
    "\n",
    "Let's try to smoothen the image and get rid of the many local minima. How wide should the gaussian kernel be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import img_as_float\n",
    "import matplotlib.image as mpimg\n",
    "from skimage import filters\n",
    "\n",
    "from skimage.util import invert\n",
    "from skimage.morphology import watershed\n",
    "\n",
    "image_nuclei = img_as_float(mpimg.imread(\"../Data/xy040-1.png\"))\n",
    "\n",
    "TriThresh = filters.threshold_triangle(image_nuclei)\n",
    "\n",
    "#Calculate mask\n",
    "mask = filters.gaussian(image_nuclei, sigma=1)>TriThresh\n",
    "\n",
    "#mask, smooth, and invert the image\n",
    "masked_image = image_nuclei*mask\n",
    "sigma_for_smoothing = \n",
    "smoothed_masked_image = \n",
    "inverted_smoothed_masked_image = invert(smoothed_masked_image)\n",
    "\n",
    "image_to_watershed = inverted_smoothed_masked_image\n",
    "#Calculate watershed transform\n",
    "#pass the mask to the watershed function so it avoids segmenting the BG\n",
    "labels_masked_smooth = watershed(image_to_watershed, watershed_line = 1, mask=mask)\n",
    "\n",
    "#let's look at all the boundaries\n",
    "plt.figure(figsize=(12,20))\n",
    "plt.subplot(211)\n",
    "plt.gca().set_title('image fed to watershed')\n",
    "plt.imshow(image_to_watershed, cmap='gray')\n",
    "plt.subplot(212)\n",
    "plt.gca().set_title('watershed result')\n",
    "plt.imshow(filters.sobel(labels_masked_smooth), cmap='nipy_spectral')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We're starting to get somewhere!! Can we do better?\n",
    "\n",
    "#### We can do more to help the algorithm by providing local markers (seeds) from which to start the process\n",
    "\n",
    "#### We will find seeds by calculating local maxima over areas that are larger than 30x30 pixels using the footprint argument for the function peak_local_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import img_as_float\n",
    "import matplotlib.image as mpimg\n",
    "from skimage import filters\n",
    "from skimage import measure\n",
    "\n",
    "from skimage.util import invert\n",
    "from skimage.morphology import watershed\n",
    "\n",
    "from skimage.feature import peak_local_max\n",
    "\n",
    "image_nuclei = img_as_float(mpimg.imread(\"../Data/xy040-1.png\"))\n",
    "\n",
    "\n",
    "mask = filters.gaussian(image_nuclei, sigma=1)>TriThresh\n",
    "\n",
    "\n",
    "#mask, smooth, and invert the image\n",
    "masked_image = image_nuclei*mask\n",
    "sigma_for_smoothing = 4\n",
    "smoothed_masked_image = filters.gaussian(masked_image, sigma=sigma_for_smoothing)\n",
    "inverted_smoothed_masked_image = invert(smoothed_masked_image)\n",
    "\n",
    "image_to_watershed = inverted_smoothed_masked_image\n",
    "\n",
    "#find local peaks to use as seeds\n",
    "#focus on this function. Look at the effect of different arguments!\n",
    "#Specifically. look at the footprint argument\n",
    "MaskedImagePeaks = \n",
    "\n",
    "\n",
    "#This is for presentation of our markers\n",
    "#create disk structuring element of radius 5\n",
    "selem = disk(5)\n",
    "#dilate local peaks so that close ones merge\n",
    "peakMask = dilation(MaskedImagePeaks,selem)\n",
    "# label local peak regions to find initial markers\n",
    "markers = measure.label(peakMask)\n",
    "\n",
    "#pass the *markers* argument to the watershed function\n",
    "labels_localmax_markers = watershed(image_to_watershed,markers=markers, watershed_line = 1, mask=mask)\n",
    "\n",
    "#let's look at all the boundaries\n",
    "plt.figure(figsize=(12,20))\n",
    "plt.subplot(211)\n",
    "plt.gca().set_title('image fed to watershed')\n",
    "plt.imshow(image_to_watershed-peakMask, cmap='gray')\n",
    "plt.clim((0.95, 1))\n",
    "plt.subplot(212)\n",
    "plt.gca().set_title('watershed result')\n",
    "plt.imshow(filters.sobel(labels_localmax_markers), cmap='nipy_spectral')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is pretty good! We're still getting a few errors here and there, but there's no big systematic over- or under- segmentation. This is a typical good result when dealing with real data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature extraction\n",
    "### Feature extraction is a process of dimensionality reduction by which an initial raw image is reduced to a list of objects and attributes\n",
    "\n",
    "<img src=\"./images/feat_ext.png\" width=\"600\" height=\"400\" >\n",
    "\n",
    "## 3.1 Extracting region properties\n",
    "\n",
    "### [scikit-image's measure module](https://scikit-image.org/docs/dev/api/skimage.measure.html)  implements a  method called *regionprops* that accepts a labeled mask of connected components, and, optionally, a corresponding image, and returns a list.  Each object on the list contains useful data about the size, shape, position, and intensity ([see the full list here](https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.regionprops)) of a specific component. \n",
    "\n",
    "The length of the list is equal to the total number of objects detected. \n",
    "\n",
    "\n",
    "#### We'll start by extracting the number of CC we found and the area of each CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "\n",
    "#We use regionprops to extract properties on all the CCs\n",
    "props = measure.regionprops(labels_localmax_markers,image_nuclei)\n",
    "#how many total connected components did we get?\n",
    "print(len(props))\n",
    "props[1].perimeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is how we make a list of a specific property for each CC\n",
    "areas = [r.area for r in props]\n",
    "\n",
    "#Do the same for the \"mean_intensity\" property\n",
    "intensities = [r.mean_intensity for r in props]\n",
    "\n",
    "#let's look at all the boundaries\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.gca().set_title('areas')\n",
    "plt.hist(areas)\n",
    "plt.subplot(122)\n",
    "plt.gca().set_title('intensities')\n",
    "plt.hist(intensities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Some options for data presentation\n",
    "\n",
    "**We can look at individual objects we found**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=2\n",
    "plt.imshow(props[i].intensity_image)\n",
    "plt.gca().set_title('Single cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's use a scatter plot to compare our results to the image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities = np.array([r.mean_intensity for r in props])\n",
    "centroids = np.array([r.centroid for r in props])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12,15))\n",
    "fig.add_axes([0.1,0.6,0.4,0.25])\n",
    "plt.gca().set_title('original')\n",
    "plt.imshow(image_nuclei, interpolation='nearest', cmap=plt.cm.gray, vmin=0, vmax=0.02)\n",
    "fig.add_axes([0.6,0.6,0.4,0.25])\n",
    "plt.scatter(centroids[:,1],centroids[:,0], c=intensities)\n",
    "plt.axis('equal')\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Or even nicer scatter plots!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities = np.array([r.mean_intensity for r in props])\n",
    "areas = np.array([r.area for r in props])\n",
    "\n",
    "centroids = np.array([r.centroid for r in props])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12,15))\n",
    "fig.add_axes([0.1,0.6,0.4,0.25])\n",
    "plt.gca().set_title('original')\n",
    "plt.imshow(image_nuclei, interpolation='nearest', cmap=plt.cm.gray, vmin=0, vmax=0.02)\n",
    "fig.add_axes([0.6,0.6,0.4,0.25])\n",
    "plt.scatter(centroids[:,1],centroids[:,0], c=intensities, s=areas/20)\n",
    "plt.axis('equal')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.text(centroids[10,1],centroids[10,0],props[10].label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can even draw your points directly on the image!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities = np.array([r.mean_intensity for r in props])\n",
    "areas = np.array([r.area for r in props])\n",
    "\n",
    "centroids = np.array([r.centroid for r in props])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12,15))\n",
    "fig.add_axes([0.1,0.6,0.8,0.5])\n",
    "plt.gca().set_title('original')\n",
    "plt.imshow(image_nuclei, interpolation='nearest', cmap=plt.cm.gray, vmin=0, vmax=0.02)\n",
    "plt.gca().patch.set_alpha(0.5)\n",
    "plt.scatter(centroids[:,1],centroids[:,0], c=intensities, alpha=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Converting regionprops to a table (Dataframe)\n",
    "\n",
    "#### Let's define some useful functions for converting a list of props into a pandas dataframe. These should become obsolete soon since the new version of scikit-image will have this functionality #### These are some useful functions for converting a list of props into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def scalar_attributes_list(im_props):\n",
    "    \"\"\"\n",
    "    Makes list of all scalar, non-dunder, non-hidden\n",
    "    attributes of skimage.measure.regionprops object\n",
    "    \"\"\"\n",
    "    \n",
    "    attributes_list = []\n",
    "    \n",
    "    for i, test_attribute in enumerate(dir(im_props[0])):\n",
    "        \n",
    "        #Attribute should not start with _ and cannot return an array\n",
    "        #does not yet return tuples\n",
    "        if test_attribute[:1] != '_' and not\\\n",
    "                isinstance(getattr(im_props[0], test_attribute), np.ndarray):                \n",
    "            attributes_list += [test_attribute]\n",
    "            \n",
    "    return attributes_list\n",
    "\n",
    "\n",
    "def regionprops_to_df(im_props):\n",
    "    \"\"\"\n",
    "    Read content of all attributes for every item in a list\n",
    "    output by skimage.measure.regionprops\n",
    "    \"\"\"\n",
    "\n",
    "    attributes_list = scalar_attributes_list(im_props)\n",
    "\n",
    "    # Initialise list of lists for parsed data\n",
    "    parsed_data = []\n",
    "\n",
    "    # Put data from im_props into list of lists\n",
    "    for i, _ in enumerate(im_props):\n",
    "        parsed_data += [[]]\n",
    "        \n",
    "        for j in range(len(attributes_list)):\n",
    "            parsed_data[i] += [getattr(im_props[i], attributes_list[j])]\n",
    "\n",
    "    # Return as a Pandas DataFrame\n",
    "    return pd.DataFrame(parsed_data, columns=attributes_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, to get all the properties in table form we simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props_df = regionprops_to_df(props)\n",
    "props_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, if we imaged our cells in multiple channels, we would want to use the same segmented nuclei and measure intensities of other channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "from skimage import img_as_float\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "image_2ndChannel = img_as_float(mpimg.imread(\"../Data/xy040-2.png\"))\n",
    "\n",
    "# extract regionprops using labels_localmax_markers mask from image_2ndChannel\n",
    "props_other_channel = measure.regionprops(labels_localmax_markers,image_2ndChannel)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.gca().set_title('Nuclei')\n",
    "plt.imshow(image_nuclei, cmap='gray')\n",
    "plt.subplot(122)\n",
    "plt.gca().set_title('other channel')\n",
    "plt.imshow(image_2ndChannel, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract only the intensity related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_2nd_channel = [r.mean_intensity for r in props_other_channel]\n",
    "max_2nd_channel = [r.max_intensity for r in props_other_channel]\n",
    "min_2nd_channel = [r.min_intensity for r in props_other_channel]\n",
    "\n",
    "plt.gca().set_title('intensities of 2nd channel')\n",
    "plt.hist(mean_2nd_channel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add these new features to the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props_df['mean_intensity_ch2'] = mean_2nd_channel\n",
    "props_df['max_intensity_ch2'] = max_2nd_channel\n",
    "props_df['min_intensity_ch2'] = min_2nd_channel\n",
    "\n",
    "props_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sometimes it's easier to see a bimodal distribution in log scale**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "fig.add_axes([0.1,0.1,0.4,0.4])\n",
    "plt.gca().set_title('Histogram of intensities')\n",
    "plt.hist(props_df.mean_intensity_ch2,20)\n",
    "fig.add_axes([0.6,0.1,0.4,0.4])\n",
    "plt.gca().set_title('Histogram of log of intensities')\n",
    "\n",
    "plt.hist(np.log(props_df.mean_intensity_ch2),20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can compare distributions of different channels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.gca().set_title('scatter plot of intensities')\n",
    "plt.scatter(np.log(props_df['max_intensity_ch2']), np.log(props_df['max_intensity']))\n",
    "plt.xlabel('Ch2')\n",
    "plt.ylabel('Ch1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Intro to tracking\n",
    "### Sometimes we image the same object at different times. It's useful to have a way to track the object and see how it changes dynamically. \n",
    "\n",
    "<img src=\"./images/procpl_0.png\" width=\"400\" height=\"400\" >\n",
    "\n",
    "### It is usually more efficient to track objects that have been segmented by using the extracted features, rather than the raw image data. As such, this is technically not an image processing, but rather a data analysis method. However, it's a very common task in microscopy experiments. \n",
    "\n",
    "### While we won't have time to build a full tracking pipeline, we will try to understand the basics of object tracking. The core building block of a tracking pipeline is a method that matches detected. objects from 2 different timepoints.\n",
    "\n",
    "\n",
    "\n",
    "**Let's start by creating a small toy dataset. The dataset will have 2 timepoints (before and after). We will  generate random positions for the \"before\" points. We will then generate random velocities and calculate the \"after\" points. Our goal is to match \"before\" points to the correct \"after\" points:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "#define region of interest\n",
    "imgx = 300\n",
    "imgy = 200\n",
    "\n",
    "#number of objects\n",
    "n=20\n",
    "\n",
    "# these are parameters for normal distributions of the velocity. \n",
    "# mux/muy correspont to drift velocity and sigmax/sigmay to diffusion.\n",
    "# We'll start with no drift and minimum diffusion. Easy!\n",
    "mux=0\n",
    "muy=0\n",
    "sigmax=1\n",
    "sigmay=1\n",
    "\n",
    "random.seed(12345678)\n",
    "\n",
    "cxList = random.randint(0, imgx ,size=n) # circle center x\n",
    "cyList = random.randint(0, imgy ,size=n) # circle center y\n",
    "vxList = mux+sigmax*random.normal(size=n) # velocity in x\n",
    "vyList = muy+sigmay*random.normal(size=n) # velocity in y\n",
    "\n",
    "centroids_t0 = np.column_stack((cxList, cyList))\n",
    "centroids_t1 = np.column_stack((cxList+vxList, cyList+vyList))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure(figsize=(12,15))\n",
    "ax = fig.add_axes([0.1,0.6,0.8,0.5])\n",
    "ax.scatter(centroids_t0[:,0], centroids_t0[:,1], s=400)\n",
    "ax.scatter(centroids_t1[:,0], centroids_t1[:,1], s=400)\n",
    "ax.legend(['t0','t1'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Nearest neighbor tracking\n",
    "\n",
    "### This is the simplest thing you could do. For every point, find the nearest neighbor and match them:\n",
    "\n",
    "<img src=\"./images/NNMatch1.png\" width=\"600\" height=\"400\" >\n",
    "\n",
    "### Notice that matching 0->1 is not always the same as matching 1->0, so make sure to operate on the right axes:\n",
    "\n",
    "<img src=\"./images/NNMatch2.png\" width=\"600\" height=\"400\" >\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### You could use an efficient method like [KdTree](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html) to calculate the nearest neighbors. In our case we have a small dataset, so we can just calculate directly. We'll use the distance_matrix method from [scipy.spatial](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance_matrix.html#scipy.spatial.distance_matrix) and the function argmin from numpy to find the matrix index where the distance is minimal. Make sure to specify the axis for argmin so it applies line by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "# The centroid positions for the 2 timepoints are stored in the variables centroids_t0 and centroids_t1\n",
    "# First calculate the distance matrix:\n",
    "dist_mat = \n",
    "correct_match = np.arange(n)\n",
    "\n",
    "# Then find the matching indices using np.argmin\n",
    "calculated_match = \n",
    "\n",
    "# Calculate the truth table for presentation\n",
    "truthT=np.array([calculated_match==c for c in correct_match])\n",
    "\n",
    "fig = plt.figure(figsize=(12,15))\n",
    "ax = fig.add_axes([0.1,0.1,0.3,0.5])\n",
    "plt.imshow(truthT)\n",
    "ax = fig.add_axes([0.5,0.175,0.6,0.35])\n",
    "ax.scatter(centroids_t0[:,0], centroids_t0[:,1], s=400)\n",
    "ax.scatter(centroids_t1[:,0], centroids_t1[:,1], s=400)\n",
    "ax.legend(['t0','t1'])\n",
    "\n",
    "ax.plot([centroids_t0[calculated_match,0], centroids_t1[:,0]],[centroids_t0[calculated_match,1], centroids_t1[:,1]] )\n",
    "\n",
    "\n",
    "print('got ' + str(100*np.trace(truthT)/truthT.shape[0]) +'% right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We're great at this! We can go home now! Let's try some more diffusion just for fun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "#define region of interest\n",
    "imgx = 300\n",
    "imgy = 200\n",
    "\n",
    "#number of objects\n",
    "n=20\n",
    "\n",
    "# these are parameters for normal distributions of the velocity. \n",
    "# mux/muy correspont to drift velocity and sigmax/sigmay to diffusion.\n",
    "# Let's add some diffusion\n",
    "mux=0\n",
    "muy=0\n",
    "sigmax=25 #<---------- This is higher now!\n",
    "sigmay=1\n",
    "\n",
    "random.seed(12345678)\n",
    "\n",
    "cxList = random.randint(0, imgx ,size=n) # circle center x\n",
    "cyList = random.randint(0, imgy ,size=n) # circle center y\n",
    "vxList = mux+sigmax*random.normal(size=n) # velocity in x\n",
    "vyList = muy+sigmay*random.normal(size=n) # velocity in y\n",
    "\n",
    "centroids_t0 = np.column_stack((cxList, cyList))\n",
    "centroids_t1 = np.column_stack((cxList+vxList, cyList+vyList))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure(figsize=(12,15))\n",
    "ax = fig.add_axes([0.1,0.6,0.8,0.5])\n",
    "ax.scatter(centroids_t0[:,0], centroids_t0[:,1], s=400)\n",
    "ax.scatter(centroids_t1[:,0], centroids_t1[:,1], s=400)\n",
    "ax.legend(['t0','t1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the same NN thing again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "# The centroid positions for the 2 timepoints are stored in the variables centroids_t0 and centroids_t1\n",
    "# First calculate the distance matrix:\n",
    "dist_mat = spatial.distance_matrix(centroids_t0, centroids_t1)\n",
    "correct_match = np.arange(n)\n",
    "\n",
    "# Then find the matching indices using np.argmin\n",
    "calculated_match = np.argmin(dist_mat, axis=0)\n",
    "\n",
    "# Calculate the truth table for presentation\n",
    "truthT=np.array([calculated_match==c for c in correct_match])\n",
    "\n",
    "fig = plt.figure(figsize=(12,15))\n",
    "ax = fig.add_axes([0.1,0.1,0.3,0.5])\n",
    "plt.imshow(truthT)\n",
    "ax = fig.add_axes([0.5,0.175,0.6,0.35])\n",
    "ax.scatter(centroids_t0[:,0], centroids_t0[:,1], s=400)\n",
    "ax.scatter(centroids_t1[:,0], centroids_t1[:,1], s=400)\n",
    "ax.legend(['t0','t1'])\n",
    "\n",
    "ax.plot([centroids_t0[calculated_match,0], centroids_t1[:,0]],[centroids_t0[calculated_match,1], centroids_t1[:,1]] )\n",
    "\n",
    "print('got ' + str(100*np.trace(truthT)/truthT.shape[0]) +'% right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ok, maybe not perfect. Can we do better? What if instead of trying to match every individual object, we instead try and find a global matching that minimizes the TOTAL distance of all connections?\n",
    "\n",
    "## 4.2 Global optimization using linear assignment.\n",
    "### Let's look at the following arrangement:\n",
    "<img src=\"./images/LAPDemo1.png\" width=\"600\" height=\"400\" >\n",
    "\n",
    "### If we just match nearest neighbors we end up with this:\n",
    "<img src=\"./images/lapdemo2.png\" width=\"600\" height=\"400\" >\n",
    "\n",
    "### That's not right. Instead, what if we consider every possible configuration of connections between the points and find the one that minimized the total distance (which we will call the \"cost\" of a configuration)\n",
    "### Or in the words of Wikipedia\n",
    "<img src=\"./images/LAP_MathFormal.png\" width=\"700\" height=\"150\" >\n",
    "\n",
    "<img src=\"./images/matchinglap.gif\" width=\"600\" height=\"400\" >\n",
    "\n",
    "### Which in our case will give us:\n",
    "<img src=\"./images/lapsol.png\" width=\"600\" height=\"400\" >\n",
    "\n",
    "### There are a few good algorithms that solve the linear assignment problem efficiently. Most famous ones are called the Hungarian Algorithm and the Jonker-Volgenant (JV) algorithm. We will use the JV algorithm which is implemented in [scipy.optimize.linear_sum_assignment](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_assignment.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "dist_mat = spatial.distance_matrix(centroids_t0, centroids_t1)\n",
    "cost_mat = dist_mat\n",
    "\n",
    "#calculate linear assignment using optimize.linear_sum_assignment:\n",
    "correct_match, calculated_match= \n",
    "\n",
    "truthT=np.array([calculated_match==c for c in correct_match])\n",
    "\n",
    "fig = plt.figure(figsize=(12,15))\n",
    "ax = fig.add_axes([0.1,0.1,0.3,0.5])\n",
    "plt.imshow(truthT)\n",
    "ax = fig.add_axes([0.5,0.175,0.6,0.35])\n",
    "ax.scatter(centroids_t0[:,0], centroids_t0[:,1], s=400)\n",
    "ax.scatter(centroids_t1[:,0], centroids_t1[:,1], s=400)\n",
    "ax.legend(['t0','t1'])\n",
    "\n",
    "ax.plot([centroids_t0[calculated_match,0], centroids_t1[:,0]],[centroids_t0[calculated_match,1], centroids_t1[:,1]] )\n",
    "\n",
    "print('got ' + str(100*np.trace(truthT)/truthT.shape[0]) +'% right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play around with different drift/diffusion parameters and see how they affect performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "#define region of interest\n",
    "imgx = 300\n",
    "imgy = 200\n",
    "\n",
    "#number of objects\n",
    "n=20\n",
    "\n",
    "# these are parameters for normal distributions of the velocity. \n",
    "# mux/muy correspont to drift velocity and sigmax/sigmay to diffusion.\n",
    "# Let's add some diffusion\n",
    "mux=0\n",
    "muy=0\n",
    "sigmax=25\n",
    "sigmay=1\n",
    "\n",
    "random.seed(12345678)\n",
    "\n",
    "cxList = random.randint(0, imgx ,size=n) # circle center x\n",
    "cyList = random.randint(0, imgy ,size=n) # circle center y\n",
    "vxList = mux+sigmax*random.normal(size=n) # velocity in x\n",
    "vyList = muy+sigmay*random.normal(size=n) # velocity in y\n",
    "\n",
    "centroids_t0 = np.column_stack((cxList, cyList))\n",
    "centroids_t1 = np.column_stack((cxList+vxList, cyList+vyList))\n",
    "\n",
    "from scipy import optimize\n",
    "dist_mat = spatial.distance_matrix(centroids_t0, centroids_t1)\n",
    "cost_mat = dist_mat\n",
    "correct_match, calculated_match= optimize.linear_sum_assignment(cost_mat)\n",
    "truthT=np.array([calculated_match==c for c in correct_match])\n",
    "\n",
    "fig = plt.figure(figsize=(12,15))\n",
    "ax = fig.add_axes([0.1,0.1,0.3,0.5])\n",
    "plt.imshow(truthT)\n",
    "ax = fig.add_axes([0.5,0.175,0.6,0.35])\n",
    "ax.scatter(centroids_t0[:,0], centroids_t0[:,1], s=400)\n",
    "ax.scatter(centroids_t1[:,0], centroids_t1[:,1], s=400)\n",
    "ax.legend(['t0','t1'])\n",
    "\n",
    "ax.plot([centroids_t0[calculated_match,0], centroids_t1[:,0]],[centroids_t0[calculated_match,1], centroids_t1[:,1]] )\n",
    "\n",
    "print('got ' + str(100*np.trace(truthT)/truthT.shape[0]) +'% right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Using more object features for tracking\n",
    "\n",
    "### So far we've only looked at the position of an object. In most cases, many other features such as object size, shape, intensity, etc. can be used to improve our tracking results. How can we incorporate other information into our tracking? \n",
    "\n",
    "### Let's build a tracking dataset but this time include also an intensity feature. For each object we'll sample an intensity from a lon-normal distribution at the first time point. We'll assume that this intensity comes from a channel that is more or less stable between the timepoints and only add a noise term to get the intensities at the second timepoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy import random as random\n",
    "from PIL import Image, ImageDraw\n",
    "imgx = 300\n",
    "imgy = 200\n",
    "cr = 1\n",
    "n=20\n",
    "\n",
    "mux=0\n",
    "muy=30\n",
    "sigmax=30\n",
    "sigmay=1\n",
    "\n",
    "cxList=[]\n",
    "cyList=[]\n",
    "vxList=[]\n",
    "vyList=[]\n",
    "\n",
    "iBeforeList = []\n",
    "iAfterList = []\n",
    "\n",
    "random.seed(123456789)\n",
    "\n",
    "cxList = random.randint(0, imgx ,size=n) # circle center x\n",
    "cyList = random.randint(0, imgy ,size=n) # circle center y\n",
    "vxList = mux+sigmax*random.normal(size=n) # velocity in x\n",
    "vyList = muy+sigmay*random.normal(size=n) # velocity in y\n",
    "iBeforeArray = np.exp(10+3*random.normal(size=n))\n",
    "iAfterArray = iBeforeArray + random.normal(size=n)\n",
    "\n",
    "\n",
    "centroids_t0 = np.column_stack((cxList, cyList))\n",
    "centroids_t1 = np.column_stack((cxList+vxList, cyList+vyList))\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.set_cmap('plasma')\n",
    "\n",
    "fig = plt.figure(figsize=(12,15))\n",
    "ax = fig.add_axes([0.1,0.6,0.8,0.5])\n",
    "ax.scatter(centroids_t0[:,0], centroids_t0[:,1],s=math.pi*20**2/4, c=np.log(iBeforeArray))\n",
    "ax.scatter(centroids_t1[:,0], centroids_t1[:,1],s=math.pi*20**2/4,c=np.log(iAfterArray))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The choice of how to incorporate this information into the cost function is up to the user/developer. Different common softwares do it differently and there is no one right answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "#as before, start with a ditance matrix\n",
    "dist_mat = spatial.distance_matrix(centroids_t0, centroids_t1)\n",
    "\n",
    "# This makes it easier to calculate all the combinations \n",
    "iBeforeDim = np.expand_dims(iBeforeArray,1)\n",
    "iAfterDim = np.expand_dims(iAfterArray,1)\n",
    "\n",
    "#try to implement different terms for using intensity\n",
    "\n",
    "#intensity_penalty = 1\n",
    "\n",
    "intensity_penalty = \n",
    "\n",
    "# Make sure you add/multiply your intensity term into the cost\n",
    "cost_mat = dist_mat*/+\n",
    "\n",
    "correct_match, calculated_match= optimize.linear_sum_assignment(cost_mat)\n",
    "truthT=np.array([calculated_match==c for c in correct_match])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12,15))\n",
    "ax = fig.add_axes([0.1,0.1,0.3,0.5])\n",
    "plt.imshow(truthT)\n",
    "ax = fig.add_axes([0.5,0.175,0.6,0.35])\n",
    "ax.scatter(centroids_t0[:,0], centroids_t0[:,1], s=400, c=np.log(iBeforeArray))\n",
    "ax.scatter(centroids_t1[:,0], centroids_t1[:,1], s=400, c=np.log(iAfterArray))\n",
    "ax.legend(['t0','t1'])\n",
    "\n",
    "ax.plot([centroids_t0[calculated_match,0], centroids_t1[:,0]],[centroids_t0[calculated_match,1], centroids_t1[:,1]] )\n",
    "\n",
    "print('got ' + str(100*np.trace(truthT)/truthT.shape[0]) +'% right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In principal, any feature can be added to your cost function. It is also not sensitive to dimensionality so tracking cells in 3D works the same and is equally efficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
